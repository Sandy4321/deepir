{
 "metadata": {
  "name": "",
  "signature": "sha256:3a46b0f280b70217e66ff69f92ece9681a257c885f583fe36dcd642e41f42b65"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Document classification by inversion of distributed language representations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The skip-gram probabalistic language model is trained to, for window $b$, maximize for each word\n",
      "\n",
      "$$\n",
      "\\prod_{k\\neq t,~k=t-b}^{t+b} \\mathrm{p}(w_k\\mid w_t)\n",
      "$$\n",
      "\n",
      "This probability is calculated for all possible word pairs over a sentence (some defined short chunk of language).\n",
      "\n",
      "In the word2vec formulation, each probability is represented as\n",
      "\n",
      "$$\n",
      "\\mathrm{p}(w \\mid w_I) = \\prod_{j=1}^{L(w)-1} \\sigma\\left( \\mathrm{ch}\\left[\\eta(w,j+1)\\right] \\mathbf{u}_{\\eta(w,j)}^\\top \\mathbf{v}_{w_I} \\right)\n",
      "$$\n",
      "\n",
      "where \n",
      "* $\\eta(w,i)$ is the $i^{th}$ node in a binary huffman tree representation (path) for word $w$, \n",
      "* $\\sigma(x) = 1/(1 + \\exp[-x])$,\n",
      "* and $\\mathrm{ch}(\\eta) \\in \\{-1,+1\\}$ translates from left/right child to +/- one.\n",
      "\n",
      "The binary huffman tree represents each word as a series of bits, so that the probability of word $w$ given word $w_I$ can be written as the product of probabilities that each bit is either on or off (represented above through the $\\mathrm{ch}$ function)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# An example binary huffman tree.\n",
      "!dot -Tpng graphs/bht.dot -o graphs/bht.png\n",
      "from IPython.display import Image\n",
      "Image(filename='graphs/bht.png') \n",
      "\n",
      "# Note that it is a prefix tree: you know the total length after each point given bits to that point."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAETCAYAAABTBf6mAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE\nQVR4nO3deVhUdd8/8PewKKmgg7sgiYYoWm4tGiSby52J92OxqJlWCKTmQiUooCZoiRt6a/cdBFQm\nJlA3pehPC8FCNLfQCjNIBA3XEFR2gfP7w2fmcTygDLOcAd6v65rLi++cOd/PzBmHN2f5jEwQBAFE\nRERE9zGSugAiIiIyPAwIREREJMKAQERERCIm6ixcUFCA7777Tle1kBZMmDAB/fr1k7oMIiJq4WTq\nnKQ4ffp07Nq1S5f1kIamT5+OnTt3Sl0GERG1cGrtQairq4OXlxeSkpJ0VQ9pwNvbG7W1tVKXQURE\nrQDPQSAiIiIRBgQiIiISYUAgIiIiEQYEIiIiEmFAICIiIhEGBCIiIhJhQCAiIiIRBgQiIiISYUAg\nIiIiEQYEIiIiEmFAICIiIhEGBCIiIhJhQCAiIiIRBgQiIiISYUBogmvXriExMRFr1qx56BgREVFr\noZeAUFRUhPj4eHh7e2PMmDGi+wVBQFxcHLy8vBAaGoo5c+Zg586das+TmZmJ6dOnQyaTQSaTYfHi\nxcjLywMA5OXlYfHixcr7pk+fjszMzEeu8/fff0d4eDimTZuGL774otExbdq6dStkMpnW10tERNRU\nMkEQhKYu7O3tDQBISkpSe6KSkhJYWlrC3t4e586dU7kvPDwc8fHxyM7OhlwuR0lJCUaMGIHAwEAs\nWrRIrXmqqqrw2GOPwdbWFvn5+aL7bW1tUVBQgKqqKrRv316tdd5fe0Nj2nDixAk4OzujsrISamwa\nAJptHyIiovvp7RCDXC5vcLywsBAREREICAhQLiOXy+Hn54eQkBAUFxerNY+ZmRkAoF27dg3erwgF\nTQ0H96/zUWOaKikpwbfffou+fftqfd1ERETqkPwchISEBNTW1sLd3V1l3M3NDRUVFYiNjZWoMv0S\nBAERERFYsmQJDy8QEZHkTKQu4PDhwwAAa2trlXHFX9FnzpzReQ1lZWXYvHkzzp8/j19//RVdunRB\nVFQUnnzySbXWc+vWLaxevRrGxsaoqanBb7/9hqFDh2L58uWN7kFR2Lp1K3x8fNC5c2dNngoREZFW\nSB4QLl++DEB8CMLS0hIAcOHChWat948//mjSX+KCIGDevHkICQnBoEGDAAATJkzAuHHjkJeXBwsL\niybNd+fOHTzzzDOYMWMG3n//fQDA9evX4eTkhG+++QY///wzunTp0uBjjx49itraWjz33HNNe3JE\nREQ6JvkhBsUv4Ad/mSt+rqmpadZ67e3tIQiC6GZvb6+yXFZWFr744gsMHjxYeYXD999/j+vXr+PH\nH39s8nxr165FXl4eAgIClGM9evRAWFgYLly4gA8++KDBxxUXFyM2NhaLFy9u1vMkIiLSBcn3IAwa\nNAiZmZkoLS1Fr169lOMlJSUAgD59+uh0/hMnTsDBwQE5OTkarScrKwsAYG5urjI+duxYAMCRI0ca\nfNzcuXMxd+5c5ObmKseqq6sBAOfOnYOpqSkGDBigUW1ERETqkjwgODg4ALh3qOH+gKA49ODk5KTT\n+cvKynDhwgWUl5ejY8eOKvfV1dXB2Ni4SesxMrq3M6agoABDhw5Vjvfs2RMAGj23YPfu3UhOTm7w\nvsGDB2PAgAH4888/m1QDERGRtkh+iMHT0xNGRkZIT09XGc/IyICpqSlmzJih1voe1TvgwfsdHBxQ\nWVmJyMhIlfGzZ89i27ZtTZ5Xsadg7969KuOXLl0CAIwfP77Bx1VVVTV6GEQQBIYDIiKShN72IJSX\nlwMA6uvrVcatra2xbNkyREdHw9/fHxYWFrh9+zaio6MRFhamdk+AsrIyAEBFRcVD66ioqECHDh3g\n4eEBOzs7REREoKioCG5ubvj9999x/PhxfPXVVyrrqqqqUq7nwbGgoCB89dVX2Lp1K2bNmoXevXsD\nAD766CM4Ojri7bffVut5EBERSUkvASEjIwM7duwAcG8X/KZNm+Dm5obhw4cDACIiImBra4v58+fD\nxsYGubm5CA4Oxpw5c9Sa5/Dhw4iLiwNw7y/3sLAwzJo1CwMHDkRubi62b9+OoqIiAMDbb7+NN998\nE05OTjh48CAWLFiAlJQU7N27F1OmTEFCQgIsLCyQn5+PzZs3A7jX1CkqKgpjx47F559/rjL2+uuv\n4+jRo4iIiMDs2bPx5JNPwtjYGN27d8fBgwdhYiL50RwiIqIm01urZdI9bh8iItIWyc9BICIiIsPD\ngEBEREQiDAhEREQkwoBAREREIgwIREREJMKAQERERCIMCERERCTCgEBEREQiDAhEREQkwoBARERE\nIgwIREREJMKAQERERCIMCERERCTCgEBEREQiDAhEREQkwoBAREREIgwIREREJGKi7gPy8/MRExOj\ni1pIQ/n5+ejfv7/UZRARUSsgEwRBaOrC7777LjZt2qTLekhDlpaWmDlzJpYuXYrevXtLXQ4REbVQ\nagUEMmwVFRWIjIzE1q1bUVtbi3nz5iEwMBA9e/bUWw2CICA+Ph779+/HwIEDce3aNbi5uWHGjBl6\nq4GIiDTHgNAKVVdX4/PPP8fq1atx9epVTJs2DSEhIRg0aJDO5w4PD0d8fDyys7Mhl8tRUlKCESNG\nIDAwEIsWLdL5/EREpB08SbEVat++Pfz9/XH+/HnExsbixIkTGDJkCDw8PHDy5EmdzVtYWIiIiAgE\nBARALpcDAORyOfz8/BASEoLi4mKdzU1ERNrFgNCKmZqaYtasWcjJycE333yDq1ev4plnnsH48eNx\n9OhRrc+XkJCA2tpauLu7q4y7ubmhoqICsbGxWp+TiIh0gwGhDTAyMoKHhwdOnDiB77//HuXl5Xj+\n+efh5OSEPXv2aG2ew4cPAwCsra1Vxvv27QsAOHPmjNbmIiIi3WJAaGPGjRuHI0eOIDMzE3K5HFOm\nTMGIESOwfft21NfXa7Tuy5cvA4Dy8IKCpaUlAODChQsarZ+IiPSHAaGNUuw9yM7OxpNPPok333wT\nw4YNw/bt21FbW9usdVpYWAAAZDKZyrji55qaGs2KJiIivWFAaOOGDx+O7du348yZMxgxYgR8fX1h\nZ2eHLVu2oKqqSq11Ka6SKC0tVRkvKSkBAPTp00c7RRMRkc4xIBAAYMiQIdi+fTvy8vIwZcoULF26\nFP369UNkZCTKy8ubtA4HBwcA/3eoQUHxs5OTk3aLJiIinWFAIBX9+vXDli1bUFBQgLfeegsffPAB\n+vXrh/fff1+5J6Axnp6eMDIyQnp6usp4RkYGTE1N2SyJiKgFYaMkeqi///4b27Ztw9atW1FTU4M3\n33wTwcHBjR4uCAsLQ2JiIk6dOgULCwvcvn0bI0eOxKxZs7BixQo9V09ERM3FgEBNUlZWhri4OKxb\ntw7FxcWYPXs2QkNDYWNjo7KcotXyoUOHYGNjg9zcXEyYMAFz5swRnbxIRESGiwGB1FJdXY3ExERE\nRESgsLBQr22ciYhIf3gOAqmlffv2mDVrFs6ePavXNs5ERKRfDAjULA+2cb527ZqyjfORI0ekLo+I\niDTEgEAaUbRxPn78OL7//ntUVFTA0dFR622ciYhIvxgQSGvGjRuHrKwsZRvnf/7zn8o2znV1dVKX\nR0REamBAIK1T7D34+eeftdbGmYiI9IsBgXTm/jbOI0eOVGnjXFlZKXV5RET0ELzMkfSmoKAAUVFR\n+OSTT2BhYYG33noL77zzjvJLnoiIyHAwIJDeXb9+Hf/+97+xefNmmJqaYv78+Vi4cKHya6GJiEh6\nDAgkmeLiYmzdurXJbZyJiEh/NAoIBQUF+O6777RZD6lpwoQJ6Nevn9RlaOTBNs7e3t5YsWIFnnji\nCbXXVVNTg127dqn9VdVtiZ2dHVxdXaUuQ+f4+fRoreHzg3RI0MC0adMEALxJeJs+fbomm9CgVFVV\nCZ9//rnwxBNPCKampsJrr70m/P7772qt4+uvv5Z8mxj6zcTEREdb0LDw8+nRt9b0+UHap9FVDHV1\ndfDy8oIgCLxJcPPy8mpVlw0+2Mb55MmTyjbOJ06caNI6FK+H1NvGUG+JiYmt6j3zMPx8evittX1+\nkPbxMkcyOIo2zr/99puyjfOzzz7LNs5ERHrEgEAG6/42zpmZmRAEQaWNsyAIUpdIRNRqMSBQi+Dk\n5IS0tDSVNs4jR45kG2ciIh1hQKAWRbH3IDs7m22ciYh0iAGBWiRFKHiwjfO+ffukLo2IqFVgQCCt\nKyoqQnx8PLy9vTFmzBidzjVkyBBs374dv/zyC5ydnfHll1/qdD4ioraCAYG0zsrKClOnTkVycjJK\nSkr0MufgwYPx2Wef4T//+Y9e5iMiau0YEEgn5HK5JPN26tRJknmJiFobBgQiIiISYUAgIiIiEQYE\nIiIiEmFAICIiIhEGBCIiIhJhQCAiIiIRBgQiIiISkSQgNKXTnq668SUmJmLYsGGQyWQYMmQIKioq\nVO5PS0vDxIkTIZPJ8PTTTyMxMVFrc9/v5MmTcHd3h7m5Ofr06QM/Pz/8/fffOplLCuXl5QCA+vp6\niSuRhrOzM2QyWYO38+fPS10e6ZCUn29E2mQixaSKTnu+vr6wt7dv9jLN4ePjgxdffBGdO3fG2bNn\nsXjxYsTExCjvHzduHJ544gnY2toiISFBq3MrnD59GqtXr8aqVavQsWNHbNiwAbGxsbhy5QpSU1O1\nPp++ZWRkYMeOHQCAgoICbNq0CW5ubhg+fLjElelHTk4Obt26hfXr16Nbt27K8WPHjiErKwsDBgyQ\nsDrSNSk/34i0SZKAADSt056uuvFZWFgAAF544QV88skncHd3h4+Pj/J+KysrAICtra1O5k9PT8fO\nnTvRoUMHAMBnn32GvXv34ocfftDJfPrm6uoKV1dXxMXFSV2KJH799VekpaWphAMA+OGHH+Dl5SVR\nVaRPUn6+EWlLmz4HITExEb169YK/vz/y8/OV46ampgCAdu3a6WTed955RxkOFGpra/Hqq6/qZD7S\nr2nTponCQXV1NVJSUuDp6SlRVURE6pFsD4Ih6N27N5KSkuDq6opp06bh8OHDjYaCW7duYfXq1TA2\nNkZNTQ1+++03DB06FMuXL9foL4H6+nqsWLECGzduhL+/f7PXQ4btwIEDsLa2xuDBg6UuhYioSdr0\nHgTg3mGG9evX48SJEwgJCWlwmTt37uCZZ55Bx44dsXbtWmzatAk7duxAamoqRo0ahdLS0mbNnZKS\nAhcXF6xduxYffvghoqOjIQiCJk+HDFRiYiIPLxBRi9Km9yAoLF68GEeOHMHGjRvh5uaGSZMmqdy/\ndu1a5OXlISAgQDnWo0cPhIWFYfbs2fjggw+wbt06ted1cXGBvb090tPTERQUhLlz58LU1BS+vr5N\nXkdhYaHKSZZt3cmTJ6UuQaSiogK7d+/GsWPHpC5FqS28Z65evYpevXpJXQYBEAQB8fHx2L9/PwYO\nHIhr167Bzc0NM2bMkLo0eggGBAAymQzx8fH47bffMHv2bJw5c0bl/qysLACAubm5yvjYsWMBAEeO\nHGnWvHK5HHK5HA4ODujcuTNmzZqFL774Qq2AcObMGZXgQoZn3759sLGxgYODg9SlKLWF90yfPn0Y\nEAxEREQE4uPjkZ2dDblcjpKSEowYMQI3btzAokWLpC6PGtHmDzEomJub4+uvv0ZlZaXoZEEjo3sv\nU0FBgcp4z549AQCdO3fWeP5//vOfAICOHTuq9bgpU6ZAEATe/vemq74VmkhMTDS4kxOl3k76uDk6\nOkr9MhPu7eWMiIhAQECA8nwtuVwOPz8/hISEoLi4WOIKqTFtMiDU1dWp/Kvg4OCA+Ph4HDp0SGVc\nsadg7969KuOXLl0CAIwfP17jmi5fvgwA8PDw0HhdZDjKysqwd+9enn9AbVZCQgJqa2vh7u6uMu7m\n5oaKigrExsZKVBk9imQBoSmd9nTVje/69esq/97P29sbixcvVhkLCgrCkCFDsHXrVly5ckU5/tFH\nH8HR0RFvv/22WvNv3LgRn376KW7fvg0AqKysxJIlS/Dmm2+2iV2/bcnu3bvx+OOPY8iQIVKXQnok\n5eeboTl8+DAAwNraWmW8b9++ACA6pEuGQ5KAkJGRgYULFwL4v057p0+fVnuZ5vjvf/+rPMbv5+eH\nzMxM0TLr1q1T2T3ZoUMHHD16FDNmzMDs2bPx7rvvIigoCN27d8fBgwdhYqLeqRw3b97EypUrMWDA\nAAQGBmLlypUIDQ1FXFwcZDKZZk+QDIri6gVu17ZDys83Q6TYO/rg5eCWlpYAgAsXLui9JmoamaDB\ndXXe3t4AgKSkJK0VRE3H118sKSkJPj4+vFy0EW3p9eH/j4fT1+szduxYZGZmorKyEmZmZsrxyspK\ndOjQASNHjsSpU6d0WgM1T5s8B4GIiPRj0KBBACDqF1NSUgLg3tUmZJgYEIiISGcUl/cqDjUoKH52\ncnLSe03UNAwIRERtVFVVlehqLm3z9PSEkZER0tPTVcYzMjJgamrKZkkGjAGBiKiN+umnn2Bubo7x\n48fj/fffR1paGu7evavVOaytrbFs2TJER0crr9y6ffs2oqOjERYWpryaQZdsbW0hk8l4a+TWvn17\nZUPA+7GTIhFRGzVs2DCMHz8eBw8exPr167Fq1Sr07NkTrq6ucHd3h5ubG/r376/xPBEREbC1tcX8\n+fNhY2OD3NxcBAcHY86cOVp4Fo9WUFCAwMBAjBkzRi/ztTTe3t4oKioSjTMgEBG1UXK5HEFBQQgK\nCkJdXR1Onz6NtLQ0HD58GO+88w7u3LmDHj16wNnZGePGjcP48eNha2ur9jwymQy+vr5qtZHXttGj\nR7NhmZoYEIiICMbGxhg1ahRGjRqF4OBg1NbW4syZM0hLS0NaWhoWLFiAmpoa9O/fH46OjnBycsKk\nSZNEDZCo9WBAICIiERMTE5XAUF5ejqNHjyoDQ0JCAurr69G/f3+MGzdOeXuwIRK1XAwIRET0SB07\ndlSGAAC4ceMGDh06hMOHDyMrKwsxMTEwNjbG8OHDlcs5OTmpNEeiloUBgYiI1Na9e3d4eXkpj+tf\nuXIFhw8fRlpaGr788ktERkbCxMQEw4YNUwaGsWPHol27dhJXTk3FgEBERBrr3bu3SmDIz89XHo74\n5JNPEBkZiY4dO2LMmDHKwDBy5Eh+T4kBYx8EIiLSuv79+8Pf3x9JSUm4ceMGTp48iTVr1kAul+OD\nDz7A008/jZ49e8Lb2xsxMTHIycmRumR6APcgEBGRThkZGSlPeFy0aJHoComFCxeiuroavXv3hpOT\nE8aNG4cXX3xRL02UqHEMCEREpFcPXiFRUVGBI0eOKAPD3LlzeYWEAWBAICIiSXXo0EHlCom///4b\nR48eRVZWlvIcBiMjI+UVEo6OjnBxcYG5ubnElbduDAhERGRQunXrBg8PD3h4eAAArl69iszMTKSl\npWHXrl28QkJPGBCIiMig9erVq9ErJGJjYxu8QmLEiBEwMuJ5+JrQOCDk5+cjJiZGG7WQmvLz87Xy\nRSqtEd+TDTt16pTUJegVP58a15I/PxRXSPj7+6O2thbHjh3DwYMHkZ6ejpUrV2Lp0qXo27cv3Nzc\n4O7uLnW5LZZGAaFv375ITk5GQECAtuohNbm4uEhdgkGxsrKCsbEx35MPYWNjI3UJesHPp0drDZ8f\nJiYmcHR0hKOjI1asWCG6QsLPz0/qElssmSAIgtRFEBER6UJJSQksLS2RmJgIb29vqcsxSDKZrMHX\nhwdoiIio1eKlkc3HkxSJiIi0rKioCAcOHMD+/ftx6dIlHD16VOV+QRAQHx+Pb775BkOHDsXJkycx\nePBgREREoHPnzhJVrYoBgYiISMusrKwwdepU+Pr6wt7eXnT/xx9/jHnz5iE7OxvDhw/HtWvX0Ldv\nX/z111/473//K0HFYjzEQEREpAMPO7zxxRdfALj3JVcA0LNnT/To0QPff/+9XmprCgYEIiIiPVOE\nh927dwMAbt68iaKiIoO6soQBgYiISM+ioqJga2uLwMBAHD9+HKGhoViyZAm+/PJLqUtTYkAgIiLS\ns4EDB+Knn37CU089BWdnZ7Rr1w7r1q1Dp06dpC5NiQGBiIhIAhUVFZDL5XBxccG//vUvvPfee6iv\nr5e6LCUGBCIiIj376aef8PTTT+P111/HN998g+effx4bN27E8uXLpS5NiQGBiIh0rqioCPHx8fD2\n9saYMWOkLkdyISEhKC4uhouLC9q3b49du3YBMKzvkWFAICIinVP0BUhOTkZJSYnU5Uju7t27AABT\nU1MA9747pEePHgb1DZSGUwkREbVqba3tcXl5OQA0eF7BzJkzAQB79uwBAPz111+4fv06fHx89Ffg\nI7CTIhERkZZlZGRgx44dAICCggJs2rQJbm5uGD58OADA398fMpkM27Ztw88//4zCwkIsW7bMoM5B\n4Lc5EhGR3shkMtjb2+PcuXN6nZPf5tg4fpsjERERNRkDAhEREYkwIBAREZEIT1IkIqJWqa6uDj/9\n9JPUZbRYDAhERNRqlJWV4cCBA9izZw/27duHGzduSF1Si8VDDEREpBcP6wugibNnzyIyMhJOTk7o\n0qULvL298eeffyI0NBTnz5/X6lxtCfcgEBGRzj2qL4A67t69ix9++AF79uxBamoq8vPzYW5ujn/8\n4x+Ij4/HpEmT0K1bN20/hTZHFBBqamqwa9cuVFVVSVEP6YiZmRmmTZuGdu3aSV2KTvH9+2h2dnZw\ndXWVugydKygowHfffSd1GQZtwoQJ6Nevn17mcnV1haurK+Li4pr1+Fu3buHbb79FamoqDh48iJs3\nb8La2hovv/wyPDw84OTkBDMzMy1X3baJAkJqaipmz54tRS2kY506dcLLL78sdRk6xffvo5mYmCj7\nwLdmy5YtU34BDjVs+vTp2Llzp9RlNOrixYtISUlBcnIyjh07hrq6Ojz//PMICgrC5MmTMWTIEKlL\nbNVEAaG2thYAwAaLrYtMJlNu29aM79+HS0pKMqhe77pUV1cHLy8vJCUlSV2KQfL29ja4z4T6+noc\nOXIEqamp2LNnD86ePYtOnTrhxRdfRFxcHP7xj3+gR48eUpfZZvAcBCIikkxZWRn+3//7f9izZw/2\n79+PGzduoE+fPvD09MSWLVvg6OiIxx57TOoy2yQGBCIi0quioiJ89dVXSE1NRVZWFqqqqjBy5EjM\nmzcPHh4eGDFihEF97XFbxYBAREQ6JQgCfv75Z+zZswfJyck4e/Ys2rdvj/Hjx2Pz5s2YOHEiHn/8\ncanLpAcwIBARkdZVV1fju+++Q2pqKg4cOIDCwkL06tULU6ZMwdq1a+Hu7o4OHTpIXSY9BAMCERFp\nxbVr1/Dtt99iz549SE9PR0VFBUaNGoXXX3+dhw5aIAYEItIJQRAQHx+P/fv3Y+DAgbh27Rrc3Nww\nY8YMqUsjLTp79iySkpKQmpqK06dPw9jYGBMmTEBUVBTGjx8PW1tbqUukZmJAICKdiIiIQHx8PLKz\nsyGXy1FSUoIRI0bgxo0bWLRokdTlUTM11MVQLpdj8uTJWLlyJdzc3NCxY0epyyQtYEAgIq0rLCxE\nREQEwsPDIZfLAQByuRx+fn4ICQnBzJkz0bVrV4mrpKZqqIvh4MGD4eXlhcmTJ2PMmDEwNjaWusxG\nGRsbw8fHp830AGkOExNxHGBAICKtS0hIQG1tLdzd3VXG3dzcEBYWhtjYWAQHB0tUHTVFbm4uUlJS\nsGfPHhw7dgwymQzOzs5YsWIFPDw80L9/f6lLbLL09HRcu3ZN6jIMlrGxMSZNmiQaZ0AgIq07fPgw\nAMDa2lplvG/fvgCAM2fO6L0meriGuhh27twZU6ZMwaJFi+Du7g5LS0upy2yWsWPHSl1Ci8SAQERa\nd/nyZQBQHl5QUPyCuXDhgt5rIrHa2lokJyerdDEcOHAgpk6diujoaIM/dEC6xYBARFpnYWEB4N53\ngNxP8XNNTY3eayKxH3/8ESkpKRg2bBj8/f0xZcoUPP3007wUkQAwIBCRDgwaNAiZmZkoLS1Fr169\nlOMlJSUAgD59+khVGt1n+PDhiIuLYxdDahBjIhFpnYODA4D/O9SgoPjZyclJ7zWRmKWlJcMBNapN\nBIRbt27pfI67d+8qT8zSlcLCQmzduhWRkZHIy8vT6VykXW1t23l6esLIyAjp6ekq4xkZGTA1NWWz\nJKIWQCsBQRAExMXFwcvLC6GhoZgzZw527typ9jJNkZGRAZlMBgsLCzz11FN47rnnIJPJYGZmhuee\new5Dhw6FmZkZZDIZ3nnnHYwdO1an11vfvHkToaGhsLS0xAsvvKCTOcrLy/Huu+9i3LhxePLJJxEU\nFAQ7OzudzEXa1Va3nbW1NZYtW4bo6Gjcvn0bAHD79m1ER0cjLCxMeTVDa+fs7AyZTNbg7fz581KX\nR/RQWjkHoSkd07TVVa2iogIuLi5ITU1VduuSyWTo168fjh07BgAoLi7G6NGj4evri88++wx1dXXa\neJoNsrS0xOrVqxETE4OysjKtr7+0tBSTJk1CcXExjh49im7duml9DtKNtr7tIiIiYGtri/nz58PG\nxga5ubkIDg7GnDlzpC5NL3JycnDr1i2sX79eZdsfO3YMWVlZGDBggITVET2axgGhKR3TysrKtNZV\nrbKyEkFBQQ9t5dm1a1fMmzcPANCjRw/liVG6IpPJ0LVrV/z9999aX7efn5/yA6Wt/YJp6dr6tpPJ\nZPD19YWvr6/UpUji119/RVpammjb//DDD/Dy8pKoKqKm0/gQw8M6plVUVCA2NrZJyzTVpEmTMH78\n+EcuN3fu3Ba/Kzc9PR1fffUVJk6ciNGjR0tdDqmB246mTZsmCgfV1dVISUmBp6enRFURNZ3GAaEp\nHdO02VWtQ4cODfaMfpCZmRnatWun/PnSpUuYOHEiLCws8Mwzz+CXX35R3pebm4upU6di2bJlmDlz\nJpydnXHmzBkIgoBvv/0W/v7+sLKywvXr1zF16lRYWFjg2Wefxa+//tro/D3eaHUAABHwSURBVBs2\nbEC7du0QGBiIzMxMAPe+9WzChAkIDQ3FkiVLYGRkhDt37jS6js8//xwAYGVlheeeew7m5uYYM2YM\nDh069MjnT9LitqOGHDhwANbW1hg8eLDUpRA9ksaHGJrSMa2ysvKRy+hadHQ0oqOjkZOTg8mTJ2P+\n/PnKX9yTJ09GXV0dUlJScPfuXXTv3h3Tp09HTk4ORo0apTxMEh0djc2bNyMnJwcvvfQS3nrrLWRl\nZYnmunnzJk6fPo2TJ0/iqaeeUo57eXnh6tWrOHDgAGQyGfLy8lBZWQlzc/MGa1as+5lnnsGmTZuQ\nk5MDLy8vuLu74/Tp03jyySd18EqRNnDbUUMSExN5eIFaDI0DQlM6phlCV7Xw8HAYGRnBxsYGlpaW\nOHXqlPK+d999V9k5zNjYGF27dkVeXh5kMhmsra3Rp08f5ObmYvny5QAAGxsb9OzZEydPnhTNk5+f\njzVr1iAqKgrdu3dXue/69eu4efMmtmzZgoULFyIiIgJmZmaN1lxUVIRevXrB398fADB69Gh8+OGH\neO211xAVFYX4+Hi1XgNF45rWrKFtIgVtbztti4mJkXR+fbh69apKkyapVVRUYPfu3cqTqYkMncYB\noSkd03r37i15VzVFADAyMkL37t3xxx9/KO8LCAjArVu3sGXLFpSWlqK6uhq1tbXK+xsKNl26dGnw\n28FeeuklDBs2rMGT0v7zn//g9ddfR2BgIHbs2IFt27Ypw1ND5HK56HCKq6srgHtnSKsrISFB5yds\n0j3a3nbaFhAQIHUJOtenTx+DCgj79u2DjY2NsokUkaHT+ByEpnRMM/SuapmZmRgyZAjs7OywcuVK\ndOrUqdnr2rBhAxITExEZGSm6z9PTE9nZ2XB3d8epU6fg5OT00L8kBw4ciOvXr0MQBOWYIng0p8aP\nP/4YgiC06ltiYqLar4suaHvbaZvU20kfN0dHR6lfZhWJiYk8OZFaFI0DQlM6pumyq9r9H8DN9cYb\nb0Amkym/D1vRN6E5637ppZcQEhKCkJAQ7Nu3T+W+NWvWwM7ODmlpadi5cyfq6uqUhy0a8vLLL6O6\nuhqnT59Wjt24cQMA8Oyzz6pdG+kPtx3dr6ysDHv37uX5B9SiaBwQmtIxTZdd1crLywFAeSLkgxTN\ni+6/WkBRg+K+mzdv4vLly8jKykJsbKyyNfPx48dx6dIlVFVVAVANDIr1Kc6fUPxbX1+PVatWwcXF\nBTNmzEB2drbyMVFRUcpfEj4+PujSpctD+6AHBASgf//+WL9+vXLulJQU9OjRA0uWLHn0i0OS4baj\n++3evRuPP/44hgwZInUpRE2mlVbLERERWLp0KebPn4/Q0FD4+voiODhY5a/jpiyjrgMHDmDBggUA\ngIsXL2Lp0qU4cuQIgHu/qDds2ICioiIAwPLly1FWVoZ169bhypUrAIAVK1aguroaGzZsQOfOnTF/\n/nzY29tj1apVkMvlWLFiBeLj41FYWKh8Drdu3cLmzZuVh0fmzp2L5cuXK6/E+PDDD3H16lW8/vrr\nuHXrFpydnfHhhx+itLQUxcXFePbZZxEeHo5FixbB2dkZu3btavT5tW/fHkePHoWRkRFee+01hIWF\n4dixYzh58qTyChAyTNx2dD/F1QsPns9EZMhkwgP70ZOSkuDj46OVXfdkOGQyGRITE+Ht7S11KTrF\n9+/DtaXXR/FeT0pKkrgSw8TXhx6lTXybIxEREamHAYGIiIhEGBCIiIhIhAGBiIiIRBgQiIiISIQB\ngYiIiEQYEIiIiEiEAYGIiIhEGBCIiIhIhAGBiIiIRBgQiIiISIQBgYiIiERMpC6AiFqvoqIiHDhw\nAPv378elS5dw9OhRqUsioibiHgQi0hkrKytMnToVycnJKCkp0du8Fy9exPXr1/U2H1FrxIBARDol\nl8v1NtedO3cQGhqKQYMG4ezZs3qbl6g1YkAgohavuroakZGR6NevHz799FN8/PHHcHZ2lrosohaN\nAYGIWrTk5GQ89dRTWL16NYKCgvDnn39i1qxZkMlkUpdG1KI1epJiTEyMPusg0iq+fxt26tQpqUvQ\nmuzsbLz77rv48ccf4evrixUrVsDKykplmfz8fL4XGpGfn4/+/ftLXQYZMFFAsLKygrGxMQICAqSo\nh3TE2NhY9OHZGvH9+2g2NjZSl6CRixcv4r333sPXX3+NF154AceOHcOoUaNEy/Xt2xfJycl8LzyE\ni4uL1CWQAZMJgiBIXQQRtW4ymQz29vY4d+5cs9dx584drFmzBlu3bkWfPn2wadMmeHh4aLFKIrof\n+yAQkUGrq6tDXFwcwsPDUV1djaioKLzxxhswNTWVujSiVo0nKRKRwfrhhx/w7LPPYuHChZg5cyb+\n+OMP+Pv7MxwQ6QEDAhHpVHl5OQCgvr6+yY/5888/4eHhARcXFwwYMAC//PIL1q5dC0tLS12VSUQP\nYEAgIp3JyMjAwoULAQAFBQXYtGkTTp8+3ejyN27cQEBAABwcHFBUVIT09HQkJSVh4MCB+iqZiP4X\nT1IkIslVV1dj8+bNiIyMhLm5OTZs2IBXXnkFRkb8G4ZIKjxJkYgklZycjNDQUFy9ehWhoaFYsGAB\nOnToIHVZRG0e4zkRSSI7Oxtubm6YPn06XF1d8fvvvyM4OJjhgMhAMCAQkV4VFhbC29sbo0aNQn19\nPY4fP47o6Og20ciLqCVhQCAivbhz5w6WLl0KBwcHZGdn49tvv8WhQ4cwcuRIqUsjogbwHAQi0ilF\no6NVq1bh7t27bHRE1EJwDwIR6cyhQ4eUjY5ee+01nDt3jo2OiFoIBgQi0rqcnByMHz8erq6uGDBg\nAH799Vc2OiJqYRgQiEhrFI2ORowYgeLiYmWjIzs7O6lLIyI18RwEItLYg42OEhIS2OiIqIVjQCAi\njbDREVHrxHhPRM3yYKOjc+fOsdERUSvCgEBEamms0VGfPn2kLo2ItIgBgYiahI2OiNoWnoNARA/F\nRkdEbRP3IBBRox5sdPTHH3+w0RFRG8GAQEQijTU6ksvlUpdGRHoiOsRQU1ODXbt2oaqqSop6SEfM\nzMwwbdo0tGvXTupSdIrv30ezs7ODq6trg/fduHEDYWFh+PTTTzF06FCkp6c3uiwRtW6igJCamorZ\ns2dLUQvpWKdOnfDyyy9LXYZO8f37aCYmJrh7967K2P2NjiwsLJCQkABPT0/IZDKJqiQiqYkCQm1t\nLQBAEAS9F0O6I5PJlNu2NeP79+GSkpLg4+OjMsZGR0TUEJ6DQNRGHT16FGPGjGGjIyJqEAMCURvk\n7e0NR0dHmJmZsdERETWIfRCI2qDjx48jISEB06ZN43kGRNQgBgSiNujcuXMwMzOTugwiMmA8xEDU\nBjEcENGjMCAQERGRCAMCERERiTAgEBERkQgDAhEREYkwIBAREZEIAwIRERGJMCAQERGRCAMCERER\niTAgEBERkQgDwiOUlpZqZRltKiws1Ot8pD3cdkTUUmglIAiCgLi4OHh5eSE0NBRz5szBzp07RcsV\nFRUhPj4e3t7eGDNmTLPmWrx4MXr37g2ZTAYTExNMnjwZEyZMwNNPP42JEyciOTkZgiBo9Hyqqqqw\nZs0ajBkzBl27dm32MtqwdetWyGQylVtERITO5iPt4bYjopZMJjzw2zQpKQk+Pj5q/ZINDw9HfHw8\nsrOzIZfLUVJSghEjRiAwMBCLFi1SWbakpASWlpawt7fHuXPnmlX0zZs30bVrV9jZ2SE3NxcAUF1d\njWXLliEqKgobN27EO++806x1K1RWVsLKygolJSWNvhZNWUYTd+/ehbOzM6ZMmaIck8lkePXVV2Ft\nba3WumQyGRITE+Ht7a3tMg1Kc96/uqDNbadNhvL6EJHh0/jbHAsLCxEREYHw8HDI5XIAgFwuh5+f\nH0JCQjBz5kyVv7AVy2jC0tISAGBk9H87QNq3b49169bh448/xkcffaRxQHjsscfQo0cPlJSUaLSM\nJr788kvMnDkT8+bN08n6SXe47YiopdP4EENCQgJqa2vh7u6uMu7m5oaKigrExsZqOkWTGRsbo1On\nTrh165be5tSV+vp6REZGIjg4GOPGjcPy5cuRn58vdVnUBNx2RNQaaBwQDh8+DACi3aZ9+/YFAJw5\nc0bTKZosKSkJN27cwBtvvKEci46OVh7/BYDbt29j48aNKmMAUFNTg+XLl2P+/PkIDQ3FkiVLUF5e\nrrL+Ry3zxRdf4LHHHoNMJsOHH36I2tpaAMDOnTvRrl07fP75501+Lrdv38bEiRMxevRo/PTTT1i9\nejUGDx6M8PDwZr02pD/cdkTUKggPSExMFBoYbtSwYcMEAEJFRYXKeHl5uQBAGD16tOgxAAR7e/sm\nz9EQAIKFhYUwe/Zs4dVXXxVGjx4tdOnSRYiOjhbq6upUlu3fv7/oOd0/VldXJ7i7uwuzZ88W6uvr\nBUEQhD///FMwNjZWaxlBEITg4GABgPDbb78px/Lz84X/+Z//afZzLS0tFSIiIpRzxcTEqL0OAEJi\nYmKza2gp1H3/6po2tp02GdrrQ0SGS+NzECwsLABA5a/x+3+uqanRdIpGde/eHatWrUJFRQUuXbqE\nlJQULFiwAOfOncP69ethbGwMADA1NRU99v6x7du34+DBgzhz5oyy7gEDBqB///7Iy8tr8jIAEBgY\niC1btiAqKkp5eGXHjh3w9fVt9vPs3LkzwsLC0K1bN8ydOxf//ve/4efnp/Z6MjMz9X5Jpr6dPHlS\n6hJUaGvbERHp3YOJQd2/MPz8/AQAwpUrV1TGi4qKBADC5MmTRY+BlvYgNLSOf/3rXwIA4YMPPlCO\n2dvbi57T/WOvvPKKAEAoLy/XaBmFt99+WzA1NRX++usvob6+XnB1dRXu3r3b/Cf7v2prawUzMzOh\nY8eOaj8WgCCXywUAbeJmaDTZdtrEPQhE1FQan4Pg4OAAALh8+bLKuOJnJycnTadQi+Iyvt27dzf5\nMRcuXACAh57c2JRlFJYsWQJBEBAVFYUTJ05g9OjRMDHReGcNjI2NYWlpCTs7u2Y9/uOPP4YgCK36\nlpiYqPHrrAuabjsiIn3TOCB4enrCyMgI6enpKuMZGRkwNTXFjBkzNJ1CLVeuXAEA9O7dWzmmOCRQ\nVVWlHFMc+hAEAU888QQA4MCBA42utynLKNjY2GDmzJmIjo7Gtm3b8Oabb6r5LBp2+fJlXL58GT4+\nPlpZH+kPtx0RtTQaBwRra2ssW7YM0dHRuH37NoB7Z3FHR0cjLCxMeTWDguKs//r6+mbPqVhHZWWl\nSsOXy5cv46233oKpqSmWLl2qHFfs5YiIiEBeXh62bdum3BNw4MABBAYGwtjYGEFBQfj+++9RWVmJ\n9PR05V6Q8+fP47333nvkMvcLCgpCWVkZLl68qAwX6ggPD8eCBQvw+++/K5/r3Llz4ePjgyVLlqi9\nPtIfbjsiag003++Ne794bW1tMX/+fNjY2CA3NxfBwcGYM2eOynIZGRnYsWMHAKCgoACbNm2Cm5sb\nhg8f3uS59u3bh127dgEALl68CEdHR5ibm6O0tBSlpaUYOXIkYmNjMXToUOVjNm7ciJKSEmzevBlp\naWmIjo6Gg4MD+vXrh5KSEri6uiI9PR1hYWF45ZVXIJfL4evri+HDh8PBwQEFBQVwcXF55DL9+vVT\nnhg5ePBguLu7Y/bs2c16TXv37o2kpCTEx8fD09MTHTp0wMKFC0X9JsjwcNsRUWuglVbLJFZTU4OR\nI0fi+PHj6NChg9TlsNUyAeDrQ0RNx29z1JGYmBhMmTLFIMIBERGRurRyiIHuOXToEN5++21UVVXh\nzp07yMnJkbokIiKiZuEeBC16/PHHcffuXRgZGSElJQXdunWTuiQiIqJm4R4ELbK1tcUff/whdRlE\nREQa4x4EIiIiEmFAICIiIhEGBCIiIhJhQCAiIiIRBgQiIiISYUAgIiIiEQYEIiIiEmFAICIiIhEG\nBCIiIhJhQCAiIiIRBgQiIiISYUAgIiIiEQYEIiIiEmn02xxjYmL0WQeRVvH927BTp05JXQIRtRCi\ngGBlZQVjY2MEBARIUQ/piLGxMaysrKQuQ+f4/n00GxsbqUsgohZAJgiCIHURREREZFh4DgIRERGJ\nMCAQERGRCAMCERERiZgASJa6CCIiIjIs/x9OvsRV7DP2IAAAAABJRU5ErkJggg==\n",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.Image at 0x7f15d8dbf7f0>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a fitted representation, we can score any new sentence (and sum across them for documents) according to the model _implied_ by the training process.  This gives a document (log) probability under that representation.  This can be calculated for language representations trained on each of the corpora associated with some class labels.  When combined with priors for each class label and inversion via Bayes rule, the document probabilities provide class probabilities."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Comparators would be any linear model regression onto phrase counts (e.g., the logistic lasso below),  the [doc2vec](http://arxiv.org/abs/1405.4053) machinery (also built into gensim), and the [Socher et al](http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) RNTN.  The doc2vec tool maps from documents to a vector space of fixed dimension, which can then be used as input to off-the-shelf machine learners.  The latter builds the sentiment/meaning into the model itself, and conditions upon this information during the training process.\n",
      "\n",
      "Advantages of the inversion framework include:\n",
      "* modularity: it works for any model of language that can (or its training can) be interpreted as a probabilistic model.\n",
      "* transparency and replicability: a simple extension of any software for training distributed language models.\n",
      "* performance (consider also 'training' the class priors, so as to correct for generative model misspecificiation).\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Example: yelp data\n",
      "\n",
      "We'll look at some proof-of-concept on the kaggle [yelp recruiting contest data](http://www.kaggle.com/c/yelp-recruiting/data), split into star-rating files via [parseyelp.py](https://github.com/TaddyLab/deepir/blob/master/code/parseyelp.py)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from copy import deepcopy\n",
      "from gensim.models import Word2Vec\n",
      "from gensim.models import Phrases\n",
      "\n",
      "fin = open(\"data/yelptrain1star.txt\")\n",
      "firstbadreview = fin.readline()\n",
      "print(firstbadreview)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " u can go there n check the car out . if u wanna buy 000 there ? thats wrong move ! if u even want a car service from there ? u made a biggest mistake of ur life !! i had 000 time asked my girlfriend to take my car there for an oil service , guess what ? they ripped my girlfriend off by lying how bad my car is now . if without fixing the problem . might bring some serious accident . then she did what they said . 000 brand new tires , timing belt , 000 new brake pads . u know whys the worst ? all of those above i had just changed 000 months before !!! what a trashy dealer is that ? people , better off go somewhere ! \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## define a review generator\n",
      "\n",
      "import re\n",
      "alteos = re.compile(r'( [!\\?] )')\n",
      "\n",
      "def revsplit(l):\n",
      "    l = alteos.sub(r' \\1 . ', l).rstrip(\"( \\. )*\\n\")\n",
      "    return [s.split() for s in l.split(\" . \")]\n",
      "\n",
      "def YelpReviews( stars = [1,2,3,4,5], prefix=\"train\" ):\n",
      "    for nstar in stars:\n",
      "        for line in open(\"data/yelp%s%dstar.txt\"%(prefix,nstar)):\n",
      "            yield revsplit(line)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## grab all sentences; good bad and ugly\n",
      "allsentences = [ s for r in YelpReviews() for s in r ]\n",
      "len(allsentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docgrp = {'neg': [1,2], 'pos': [5]} \n",
      "[g for g in docgrp]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reviews = { g: list(YelpReviews(docgrp[g])) for g in docgrp }\n",
      "ndoc = pd.Series( {g: len(reviews[g]) for g in docgrp} , dtype=\"float64\" )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reviews['neg'][0][6:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jointmodel = Word2Vec(workers=4)\n",
      "np.random.shuffle(allsentences)\n",
      "jointmodel.build_vocab(allsentences)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = { g: deepcopy(jointmodel) for g in docgrp }\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainW2V(g, T=20):\n",
      "    sent = [l for r in reviews[g] for l in r]\n",
      "    model[g].min_alpha = model[g].alpha\n",
      "    for epoch in range(T):\n",
      "        print(epoch, end=\" \")\n",
      "        np.random.shuffle(sent)\n",
      "        model[g].train(sent)\n",
      "        model[g].alpha *= 0.9  \n",
      "        model[g].min_alpha = model[g].alpha  \n",
      "    print(\".\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for g in docgrp: \n",
      "    print(g, end=\": \")\n",
      "    trainW2V( g )\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def nearby(word, g):\n",
      "    print(word)\n",
      "    print( \"%s:\"%str(g), end=\" \")\n",
      "    for (w,v) in model[g].most_similar([word]):\n",
      "        print(w, end=\" \")\n",
      "    print(\"\\n\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for g in docgrp: nearby(\"food\", g)\n",
      "for g in docgrp: nearby(\"service\", g)\n",
      "for g in docgrp: nearby(\"value\", g)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Everything to this point uses standard gensim.  For the next bit, we're using the `score` functions implemented in the [taddylab fork](https://github.com/TaddyLab/gensim)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testrev = { g: list(YelpReviews(docgrp[g], \"test\")) for g in docgrp }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getprobs(rev, grp):\n",
      "    sentences =  [(i,s) for i,r in enumerate(rev) for s in r]\n",
      "    eta = pd.DataFrame(\n",
      "            { g: model[g].score([s for i,s in sentences])  \n",
      "             for g in grp } )\n",
      "    probs = eta.subtract( eta.max('columns'), 'rows') \n",
      "    probs = np.exp( probs )\n",
      "    probs = probs.divide(probs.sum('columns'), \"rows\")\n",
      "    probs['cnt'] = 1\n",
      "    probs = probs.groupby([i for i,s in sentences]).sum()\n",
      "    probs = probs.divide(probs[\"cnt\"], 'rows').drop(\"cnt\", 1)\n",
      "    return(probs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "probs = {g: getprobs(testrev[g], docgrp) for g in docgrp }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "fig = plt.figure(figsize=(7,4))\n",
      "plt.hist(probs['neg']['pos'],normed=1,\n",
      "    color=\"red\", alpha=.6, label=\"true negative\", linewidth=1)\n",
      "plt.hist(probs['pos']['pos'],normed=1,\n",
      "    color=\"yellow\", alpha=.6, label=\"true positive\", linewidth=1)\n",
      "plt.xlim([0,1])\n",
      "plt.ylim([0,5])\n",
      "plt.legend(frameon=False, loc='upper center')\n",
      "plt.xlabel(\"prob positive\")\n",
      "plt.ylabel(\"density\")\n",
      "#fig.savefig(\"graphs/coarseprobs.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "yhat = {g: probs[g].idxmax('columns') for g in docgrp}\n",
      "mc = pd.DataFrame({\n",
      "    'mcr': {g: (yhat[g] != g).mean() for g in docgrp},\n",
      "    'n': {g: len(testrev[g]) for g in docgrp}\n",
      "    })\n",
      "print(mc)\n",
      "\n",
      "overall = mc.product(\"columns\").sum()/mc['n'].sum()\n",
      "print(\"\\nOverall MCR: %.3f\" %overall)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the fit looks nice and tight. OOS we get around 14% misclassification rate on the reviews."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svec = np.concatenate((probs['neg']['pos'],probs['pos']['pos']), axis=0)\n",
      "allrev = [[w for s in r for w in s] for r in testrev['neg']+testrev['pos']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "diff = pd.Series( svec )\n",
      "tops = diff.order(ascending=False)[:5]\n",
      "print(\"TOPS\\n\")\n",
      "for i in tops.index:\n",
      "    print( \" \".join(allrev[i]), end=\"\\n\\n\")\n",
      "bots = diff.order()[:5]\n",
      "print(\"BOTTOMS\\n\")\n",
      "for i in bots.index:\n",
      "    print( \" \".join(allrev[i]), end=\"\\n\\n\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Now, same thing but on fine scale\n",
      "\n",
      "This copies most of the code from above and replaces it with classification amongst the 1-5 ratings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docgrp_fine = {str(i) : [i] for i in range(1,6)} \n",
      "docgrp_fine"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for g in docgrp_fine:\n",
      "    print(g, end=\": \")\n",
      "    reviews[g] = list(YelpReviews(docgrp_fine[g]))\n",
      "    model[g] = deepcopy(jointmodel)\n",
      "    trainW2V( g )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for g in docgrp_fine:\n",
      "    testrev[g] =  list( YelpReviews(docgrp_fine[g], \"test\") )\n",
      "    probs[g] = getprobs(testrev[g], docgrp_fine)\n",
      "    yhat[g] = probs[g].idxmax(\"columns\")\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mc_fine = pd.DataFrame({\n",
      "    'mcr': {g: (yhat[g] != g).mean() for g in docgrp_fine},\n",
      "    'n': {g: len(testrev[g]) for g in docgrp_fine}\n",
      "    })\n",
      "print(mc_fine)\n",
      "\n",
      "ntest = mc_fine['n'].sum()\n",
      "overall_fine = mc_fine.product(\"columns\").sum()/ntest\n",
      "print(\"\\nOverall Fine-Scale MCR: %.3f\" %overall_fine)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, phrases to disk for linear model analysis.  We use R, because sci-kit learn doesn't seem have a fast L1 path algorithm for logistic regression.  The models are then fit via the [gamlr](https://github.com/TaddyLab/gamlr) package, with [optimal penalty size selected via corrected AICc](http://arxiv.org/abs/1308.5623),  following the code in [linmod.R](https://github.com/TaddyLab/deepir/blob/master/code/linmod.R)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phraser = Phrases(allsentences,threshold=5.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for w in phraser[allsentences[0]]:\n",
      "        print(w, end=\" \")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i = 0\n",
      "fout = open(\"data/yelp_phrases.txt\", \"w\")\n",
      "for samp in [\"train\",\"test\"]:\n",
      "    for stars in range(1,6):  \n",
      "        if samp == \"train\":\n",
      "            rev = reviews[str(stars)]\n",
      "        else:\n",
      "            rev = testrev[str(stars)]\n",
      "  \n",
      "        for r in rev:\n",
      "            for s in r:\n",
      "                for w in phraser[s]:\n",
      "                    if \"|\" not in w:\n",
      "                        fout.write(\"%d|%s|%d|%s\\n\" % (i,w,stars,samp))\n",
      "            i += 1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Pfine = {}\n",
      "\n",
      "for stars in range(1,6):\n",
      "    Pfine['train%d'%stars] = getprobs(reviews[str(stars)], docgrp_fine)\n",
      "    Pfine['test%d'%stars] = getprobs(testrev[str(stars)], docgrp_fine)\n",
      "    \n",
      "pmatfine = pd.concat( [Pfine['train%d'%s] for s in range(1,6)] + [Pfine['test%d'%s] for s in range(1,6)] )\n",
      "pmatfine.to_csv(\"data/fineyelpscore.csv\",index=False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Alternatively, try doc2vec analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models.doc2vec import *\n",
      "def YelpLabeledSentence( stars = [1,2,3,4,5], prefix=\"train\" ):\n",
      "    for nstar in stars:\n",
      "        i = 0\n",
      "        for line in open(\"data/yelp%s%dstar.txt\"%(prefix,nstar)):\n",
      "            line = alteos.sub(r' \\1 . ', line).rstrip(\"( \\. )*\\n\")\n",
      "            lab = \"%s-%d-%d\" % (prefix, nstar, i)\n",
      "            rev = [s.split() for s in line.split(\" . \")]\n",
      "            i += 1\n",
      "            for s in rev:\n",
      "                yield LabeledSentence(s, [lab])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainsent = list(YelpLabeledSentence()) \n",
      "testsent = list(YelpLabeledSentence(prefix=\"test\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdm0 = Doc2Vec(workers=4, size=100, window=5, dm=0)\n",
      "mdm1 = Doc2Vec(workers=4, size=100, window=5, dm=1)\n",
      "%time mdm0.build_vocab(trainsent+testsent)\n",
      "%time mdm1.build_vocab(trainsent+testsent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainD2V(mod, sent, T=20):\n",
      "    mod.min_alpha = mod.alpha\n",
      "    for epoch in range(T):\n",
      "        print(epoch, end=\" \")\n",
      "        np.random.shuffle(sent)\n",
      "        mod.train(sent)\n",
      "        mod.alpha *= 0.9  \n",
      "        mod.min_alpha = mod.alpha  \n",
      "    print(\".\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time trainD2V(mdm0, trainsent)\n",
      "%time trainD2V(mdm1, trainsent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## turn of training of word vecs, just score label vecs\n",
      "mdm0.train_words=False\n",
      "mdm1.train_words=False\n",
      "%time trainD2V(mdm0, testsent)\n",
      "%time trainD2V(mdm1, testsent)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod = mdm0\n",
      "prefix=\"train\"\n",
      "stars = 1\n",
      "\n",
      "v = []\n",
      "y = np.empty(0)\n",
      "x = np.empty([0,mod.syn0.shape[1]])\n",
      "\n",
      "labs = [ w for w in mod.vocab if re.match(\"%s-%d-\\d+\"%(prefix,stars), w) ]\n",
      "v += labs\n",
      "i = [mod.vocab[w].index for w in labs] \n",
      "y = np.append(y, np.repeat(stars,len(i)))\n",
      "x = np.vstack( (x, mod.syn0[i,:]) )\n",
      "\n",
      "veclab = [\"x%d\"%d for d in range(1,x.shape[1]+1)]\n",
      "df = pd.DataFrame( x, index=v, columns=veclab )\n",
      "df[\"stars\"] = y\n",
      "df.to_csv(\"test.csv\", index_label=\"id\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def writeD2V(mod, fname, prefix):\n",
      "    v = []\n",
      "    y = np.empty(0)\n",
      "    x = np.empty([0,mod.syn0.shape[1]])\n",
      "    for stars in range(1,6):\n",
      "        labs = [ w for w in mod.vocab if re.match(\"%s-%d-\\d+\"%(prefix,stars), w) ]\n",
      "        v += labs\n",
      "        i = [mod.vocab[w].index for w in labs] \n",
      "        y = np.append(y, np.repeat(stars,len(i)))\n",
      "        x = np.vstack( (x, mod.syn0[i,:]) )\n",
      "    \n",
      "    veclab = [\"x%d\"%d for d in range(1,x.shape[1]+1)]\n",
      "    df = pd.DataFrame( x, index=v, columns=veclab )\n",
      "    df[\"stars\"] = y\n",
      "    df.to_csv(\"data/%s.csv\"%fname, index_label=\"id\")\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for prefix in [\"train\",\"test\"]:\n",
      "    writeD2V(mdm0, \"yelpD2V%s0\"%prefix, prefix)\n",
      "    writeD2V(mdm1, \"yelpD2V%s1\"%prefix, prefix)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### linear modelling in R\n",
      "\n",
      "The rest -- forward linear and logistic modelling -- will happen in R to make sure we're comparing linear apples to linear apples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!Rscript code/linmod.R"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}